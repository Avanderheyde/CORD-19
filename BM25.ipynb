{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Open Research Dataset Challenge\n",
    "\n",
    "https://www.youtube.com/watch?v=S6GVXk6kbcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/e4/38d03d6d5e2deae8d2838b81d6ba2742475ced42045f5c46aeb00c5fb79c/rank_bm25-0.2.tar.gz\n",
      "Requirement already satisfied: nltk in /Users/Alderik/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: numpy in /Users/Alderik/anaconda3/lib/python3.7/site-packages (from rank_bm25) (1.17.2)\n",
      "Requirement already satisfied: six in /Users/Alderik/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: rank-bm25\n",
      "  Building wheel for rank-bm25 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rank-bm25: filename=rank_bm25-0.2-cp37-none-any.whl size=4163 sha256=2657ddad49320196843f207152609bd14baef5631c0deade72f1bf844333f1cd\n",
      "  Stored in directory: /Users/alderik/Library/Caches/pip/wheels/6f/0c/1f/78945dd6a5478bbcdb50d73ac96ae5af2ffcdfcd374fd9b1bf\n",
      "Successfully built rank-bm25\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from requests.exceptions import HTTPError, ConnectionError\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc530f7012a4a4ebd2a798ae0cf3f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=200, description='ColumnWidth', max=400, min=50, step=50), IntSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://www.kaggle.com/dgunning/browsing-research-papers-with-a-bm25-search-engine?scriptVersionId=31027514\n",
    "\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "\n",
    "def set_column_width(ColumnWidth, MaxRows):\n",
    "    pd.options.display.max_colwidth = ColumnWidth\n",
    "    pd.options.display.max_rows = MaxRows\n",
    "    print('Set pandas dataframe column width to', ColumnWidth, 'and max rows to', MaxRows)\n",
    "    \n",
    "interact(set_column_width, \n",
    "         ColumnWidth=widgets.IntSlider(min=50, max=400, step=50, value=200),\n",
    "         MaxRows=widgets.IntSlider(min=50, max=500, step=100, value=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import metadata\n",
    "metadata = pd.read_csv(\"metadata.csv\", dtype={'Microsoft Academic Paper ID': str,\n",
    "                                      'pubmed_id': str}) \n",
    "metadata = metadata.dropna(subset=['sha'])\n",
    "metadata.rename(columns={\"sha\": \"paper_id\"}, inplace = True)\n",
    "metadata.rename(columns={\"source_x\": \"source\"}, inplace = True)\n",
    "metadata= metadata.drop(columns = ['title','abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doi_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b598a48c39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoi_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doi_url' is not defined"
     ]
    }
   ],
   "source": [
    "for i in metadata['doi']:\n",
    "    url = doi_url(str(i))\n",
    "    metadata.loc[metadata['doi'] == i, 'doi'] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 68/885 [00:00<00:01, 679.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biorxiV_medrxiv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 885/885 [00:01<00:00, 782.17it/s]\n",
      "  1%|          | 65/9118 [00:00<00:14, 643.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_use_subset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9118/9118 [00:16<00:00, 553.67it/s]\n",
      "  0%|          | 0/16959 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_license\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16959/16959 [00:33<00:00, 505.71it/s]\n",
      "  2%|▏         | 55/2353 [00:00<00:04, 544.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noncomm_use_subset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2353/2353 [00:04<00:00, 528.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#import text from papers json objects\n",
    "#https://www.youtube.com/watch?v=S6GVXk6kbcs\n",
    "dirs = ['biorxiV_medrxiv', 'comm_use_subset', 'custom_license', 'noncomm_use_subset']\n",
    "\n",
    "docs = []\n",
    "for d in dirs:\n",
    "    print(d)\n",
    "    for file in tqdm(os.listdir(f\"{d}/{d}\")):\n",
    "        filepath = f\"{d}/{d}/{file}\"\n",
    "        j = json.load(open(filepath,'rb'))\n",
    "        title = j['metadata']['title']\n",
    "        paper_id = j['paper_id']\n",
    "        try: \n",
    "            abstract = j['abstract'][0]['text']\n",
    "        except:\n",
    "            abstract = ''\n",
    "            \n",
    "        fulltext = ''\n",
    "        for text in j['body_text']:\n",
    "            fulltext += text['text']\n",
    "        docs.append([paper_id,title, abstract, fulltext])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(docs, columns = ['paper_id','title', 'abstract', 'fulltext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Join metadata with paper text on paper_id\n",
    "allpapers_df = pd.merge(df, metadata, on=\"paper_id\")\n",
    "allpapers_df['journal'] = allpapers_df['journal'].astype(str)\n",
    "peer_reviewed = allpapers_df['journal'] !='nan'\n",
    "#Make column to say whether a paper was peer reviewed\n",
    "#basically anything from bioRxiv/medRxiv subset \n",
    "allpapers_df.insert(12, \"peer_reviewed\", peer_reviewed, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Made data frame for all journal papers\n",
    "journals_df = allpapers_df[allpapers_df['journal']!='nan']\n",
    "#dataframe for unpublished papers\n",
    "unpublished_df = allpapers_df[allpapers_df['journal']=='nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers including journals 27690 \n",
      "\n",
      "Total number of journals 26796 \n",
      "\n",
      "Number of unpublsihed papers 894\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of papers including journals {len(allpapers_df)} \\n\\nTotal number of journals {len(journals_df)} \\n\\nNumber of unpublsihed papers {len(unpublished_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Object Oriented Programming\n",
    "https://www.kaggle.com/dgunning/browsing-research-papers-with-a-bm25-search-engine?scriptVersionId=31027514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url, timeout=6):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        return r.text\n",
    "    except ConnectionError:\n",
    "        print(f'Cannot connect to {url}')\n",
    "        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n",
    "    except HTTPError:\n",
    "        print('Got http error', r.status, r.text)\n",
    "        \n",
    "# Convert the doi to a url\n",
    "def doi_url(d): \n",
    "    return f'http://{d}' if d.startswith('doi.org') else f'http://doi.org/{d}'\n",
    "\n",
    "class ResearchPapers:\n",
    "    \n",
    "    def __init__(self, metadata: pd.DataFrame):\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return Paper(self.metadata.iloc[item])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def head(self, n):\n",
    "        return ResearchPapers(self.metadata.head(n).copy().reset_index(drop=True))\n",
    "    \n",
    "    def tail(self, n):\n",
    "        return ResearchPapers(self.metadata.tail(n).copy().reset_index(drop=True))\n",
    "    \n",
    "    def abstracts(self):\n",
    "        return self.metadata.abstract\n",
    "    \n",
    "    def titles(self):\n",
    "        return self.metadata.title.dropna()\n",
    "    \n",
    "    #i added\n",
    "    def texts(self):\n",
    "        return self.metadata.fulltext\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.metadata._repr_html_()\n",
    "    \n",
    "    \n",
    "class Paper:\n",
    "    \n",
    "    '''\n",
    "    A single research paper\n",
    "    '''\n",
    "    def __init__(self, item):\n",
    "        self.paper = item.to_frame().fillna('')\n",
    "        self.paper.columns = ['Value']\n",
    "    \n",
    "    def text(self):\n",
    "        return self.paper.loc['fulltext'].values[0]\n",
    "    \n",
    "    def abstract(self):\n",
    "        return self.paper.loc['abstract'].values[0]\n",
    "    \n",
    "    def title(self):\n",
    "        return self.paper.loc['title'].values[0]\n",
    "    \n",
    "    def doi(self):\n",
    "        return self.paper.loc['doi'].values[0]\n",
    "    \n",
    "    def peer_reviewed(self):\n",
    "        return self.paper.loc['peer_reviewed'].values[0]\n",
    "    \n",
    "    def journal(self):\n",
    "        return self.paper.loc['journal'].values[0]\n",
    "    \n",
    "    def authors(self, split=False):\n",
    "        '''\n",
    "        Get a list of authors\n",
    "        '''\n",
    "        authors = self.paper.loc['authors'].values[0]\n",
    "        if not authors:\n",
    "            return []\n",
    "        if not split:\n",
    "            return authors\n",
    "        if authors.startswith('['):\n",
    "            authors = authors.lstrip('[').rstrip(']')\n",
    "            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n",
    "        \n",
    "        # Todo: Handle cases where author names are separated by \",\"\n",
    "        return [a.strip() for a in authors.split(';')]\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.paper._repr_html_()\n",
    "    \n",
    "papers = ResearchPapers(allpapers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25 Query\n",
    "https://pypi.org/project/rank-bm25/  \n",
    "http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "english_stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "def strip_characters(text):\n",
    "    t = re.sub('\\(|\\)|:|,|;|\\.|’|”|“|\\?|%|>|<', '', text)\n",
    "    t = re.sub('/', ' ', t)\n",
    "    t = t.replace(\"'\",'')\n",
    "    return t\n",
    "\n",
    "def clean(text):\n",
    "    t = text.lower()\n",
    "    t = strip_characters(t)\n",
    "    return t\n",
    "\n",
    "def tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return list(set([word for word in words \n",
    "                     if len(word) > 1\n",
    "                     and not word in english_stopwords\n",
    "                     and not (word.isnumeric() and len(word) is not 4)\n",
    "                     and (not word.isnumeric() or word.isalpha())] )\n",
    "               )\n",
    "\n",
    "def preprocess(text):\n",
    "    t = clean(text)\n",
    "    tokens = tokenize(t)\n",
    "    return tokens\n",
    "\n",
    "class SearchResults:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame,\n",
    "                 columns = None):\n",
    "        self.results = data\n",
    "        if columns:\n",
    "            self.results = self.results[columns]\n",
    "            \n",
    "    def __getitem__(self, item):\n",
    "        return Paper(self.results.loc[item])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.results)\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.results._repr_html_()\n",
    "\n",
    "SEARCH_DISPLAY_COLUMNS = ['title', 'abstract', 'doi', 'peer_reviewed', 'journal','fulltext']\n",
    "    \n",
    "class RankBM25Index:\n",
    "    \n",
    "    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n",
    "        self.corpus = corpus\n",
    "        self.columns = columns\n",
    "        raw_search_str = self.corpus.abstract + ' ' + self.corpus.title\n",
    "        self.index = raw_search_str.apply(preprocess).to_frame()\n",
    "        self.index.columns = ['terms']\n",
    "        self.index.index = self.corpus.index\n",
    "        self.bm25 = BM25Okapi(self.index.terms.tolist())\n",
    "        \n",
    "    def search(self, search_string, n=10):\n",
    "        search_terms = preprocess(search_string)\n",
    "        doc_scores = self.bm25.get_scores(search_terms)\n",
    "        ind = np.argsort(doc_scores)[::-1][:n]\n",
    "        results = self.corpus.iloc[ind][self.columns]\n",
    "        results['Score'] = doc_scores[ind]\n",
    "        results = results[results.Score > 0]\n",
    "        return SearchResults(results.reset_index(), self.columns + ['Score'])\n",
    "\n",
    "bm25_index = RankBM25Index(allpapers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is known about transmission, incubation, and environmental stability?\n",
    "results = bm25_index.search('smoking and covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>peer_reviewed</th>\n",
       "      <th>journal</th>\n",
       "      <th>fulltext</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Eff ects of smoking and solid-fuel use on COPD, lung cancer, and tuberculosis in China: a time-based, multiple risk factor, modelling study</td>\n",
       "      <td>Background Chronic obstructive pulmonary disease (COPD), lung cancer, and tuberculosis are three leading causes of death in China, where prevalences of smoking and solid-fuel use are also high. We...</td>\n",
       "      <td>10.1016/S0140-6736(08)61345-8</td>\n",
       "      <td>True</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>Chronic obstructive pulmonary disease (COPD), lung cancer, and tuberculosis are the second, sixth, and eighth leading causes of death in China, accounting for almost 2 million deaths in 2002 (20·5...</td>\n",
       "      <td>8.399632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Environmental factors and their regulation of immunity in multiple sclerosis</td>\n",
       "      <td>Epidemiological and clinical studies have shown that environmental factors such as infections, smoking and vitamin D are associated with the risk of developing multiple sclerosis (MS). Some of the...</td>\n",
       "      <td>10.1016/j.jns.2012.10.021</td>\n",
       "      <td>True</td>\n",
       "      <td>Journal of the Neurological Sciences</td>\n",
       "      <td>Multiple sclerosis (MS) is an inflammatory disease of the central nervous system (CNS) in which an interplay of genetic and environmental factors leads to the chronic activation of immune cells an...</td>\n",
       "      <td>7.727815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Bulk and single-cell transcriptomics identify tobacco-use disparity in lung gene expression of ACE2, the receptor of 2019-nCov</td>\n",
       "      <td>In current severe global emergency situation of 2019-nCov outbreak, it is imperative to identify vulnerable and susceptible groups for effective protection and care. Recently, studies found that 2...</td>\n",
       "      <td>10.1101/2020.02.05.20020107</td>\n",
       "      <td>False</td>\n",
       "      <td>nan</td>\n",
       "      <td>In the past two decades, pathogenic coronaviruses (CoVs) have caused epidemic infections, including the server acute respiratory syndrome (SARS)-CoV outbreak in 2003, the Middle East Respiratory S...</td>\n",
       "      <td>7.097105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Brief Communication Psychological Responses among Humidifier Disinfectant Disaster Victims and Their Families</td>\n",
       "      <td>To substantiate psychological symptoms following humidifier disinfectant (HD) disasters, counseling records of 26 victims and 92 family members of victims (45 were bereaved) were analyzed retrospe...</td>\n",
       "      <td>10.3346/jkms.2019.34.e29</td>\n",
       "      <td>True</td>\n",
       "      <td>J Korean Med Sci</td>\n",
       "      <td>In Korea, several types of chemical disinfectants that had been widely used in humidifiers since 1994 were found to be associated with lung injury, including interstitial pneumonitis and widesprea...</td>\n",
       "      <td>6.872736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Antimicrobial dosing considerations in obese adult patients</td>\n",
       "      <td>publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. Thes...</td>\n",
       "      <td>10.1016/S1473-3099(10)70120-1</td>\n",
       "      <td>True</td>\n",
       "      <td>The Lancet Infectious Diseases</td>\n",
       "      <td>As pandemic infl uenza H1N1 spread around the world in 2009, disease severity was one of the main areas of interest. The case fatality ratio (CFR) is a representative measurement of severity of a ...</td>\n",
       "      <td>6.631344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>The current state of cardiology in China</td>\n",
       "      <td>Cardiology in China has shown significant changes in the last decade or so. Interventional cardiology, in particular, has shown remarkable advances, especially in the management of coronary artery...</td>\n",
       "      <td>10.1016/j.ijcard.2003.10.011</td>\n",
       "      <td>True</td>\n",
       "      <td>International Journal of Cardiology</td>\n",
       "      <td>I used to return to China to lecture, demonstrate, and exchange scientific and medical information every 1 -2 years since 1972. I have previously reported my observations on several occasions [1] ...</td>\n",
       "      <td>6.464027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>World Journal of Clinical Cases CASE REPORT 222 Duodenal gangliocytic paraganglioma with lymph node metastases: A case report and comparative review of 31 cases 234 Bilateral renal cortical necros...</td>\n",
       "      <td>MINIREVIEWS</td>\n",
       "      <td>10.12998/wjcc.v5.i6.212</td>\n",
       "      <td>True</td>\n",
       "      <td>World J Clin Cases</td>\n",
       "      <td>A cross-sectional survey among the Gulf Cooperation Council (GCC) countries' residents. Data collected electronically through a smartphone app. The survey variables aimed to investigate the respon...</td>\n",
       "      <td>6.369334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Association between Serum Angiotensin-converting Enzyme 2 Level with Postoperative Morbidity and Mortality after Major Pulmonary Resection in Non-small Cell Lung Cancer Patients</td>\n",
       "      <td>After adjustment for age, sex, body mass index, current smoking status, forced expiratory volume in 1 second, coronary heart disease, hypertension, diabetes mellitus, chronic obstructive pulmonary...</td>\n",
       "      <td>10.1016/j.hlc.2013.12.013</td>\n",
       "      <td>True</td>\n",
       "      <td>Heart, Lung and Circulation</td>\n",
       "      <td>The major comorbidity in patients with non-small cell lung cancer (NSCLC) is of cardiovascular nature and is reported to be up to 23% [1] . The incidence of cardiovascular disease has been describ...</td>\n",
       "      <td>6.277375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Journal Pre-proof Prevalence of comorbidities in the novel Wuhan coronavirus (COVID-19) infection: a systematic review and meta-analysis Prevalence of comorbidities in the Novel Wuhan Coronavirus ...</td>\n",
       "      <td> COVID -19 cases now confirmed in multiple countries.</td>\n",
       "      <td>10.1016/j.ijid.2020.03.017</td>\n",
       "      <td>True</td>\n",
       "      <td>International Journal of Infectious Diseases</td>\n",
       "      <td>On December 31, 2019, a cluster of cases of \"pneumonia of unknown origin\" in people associated with the Huanan Seafood Wholesale Market has been reported in Wuhan, China. Only a few days later, Ch...</td>\n",
       "      <td>6.177175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Estimation of the final size of the second phase of the coronavirus COVID 19 epidemic by the logistic model</td>\n",
       "      <td>In the note, the logistic growth regression model is used for the estimation of the final size and its peak time of the coronavirus epidemic in China, South Korea, and the rest of the World.</td>\n",
       "      <td>10.1101/2020.03.11.20024901</td>\n",
       "      <td>False</td>\n",
       "      <td>nan</td>\n",
       "      <td>however, use approximate solution and thus obtain four-parameter problem which can be very sensitive to initial guess). Yet, the logistics model has its drawbacks as the epidemic approaches its fi...</td>\n",
       "      <td>5.976890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<__main__.SearchResults at 0x1a291290f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e5ae75bc7513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_summary' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(results)):\n",
    "    print(get_summary(results[i].text()))\n",
    "    print()\n",
    "    print()\n",
    "    break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "# Get a list of stopwords from nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # Removing Square Brackets and Extra Spaces\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    " \n",
    "    text = re.sub(r'\\{\\{[\\s\\S]*?\\}\\}', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_spchar_digs(text):\n",
    "    # Removing special characters and digits\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text )\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def word_freq(formatted_text):\n",
    "    #creates a dictionary of words as keys and frequency as values\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    #divides the values by the maximum frequency\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
    "    \n",
    "    return word_frequencies\n",
    "\n",
    "def sent_scores(sentence_list, word_frequencies):\n",
    "    #uses the word frequencies to score the sentences by adding up the scores\n",
    "    #of the words that make up the sentence\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) <60: #limits sentence to less than 60 words\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "                    \n",
    "    return sentence_scores\n",
    "\n",
    "def get_summary(dirty_text):\n",
    "    '''\n",
    "    Input: text that is already filtered for desired keywords\n",
    "    '''\n",
    "    #cleans the text\n",
    "    text = clean_text(dirty_text)\n",
    "    #remove special chars and nums\n",
    "    formatted_text = clean_spchar_digs(text)\n",
    "    #tokenize\n",
    "    sentence_list = nltk.sent_tokenize(text)\n",
    "    #wordfrequencies\n",
    "    word_frequencies = word_freq(formatted_text)\n",
    "    #dictionary of sentences with sentence as key and word frequency score as value\n",
    "    sentence_scores = sent_scores(sentence_list,word_frequencies)\n",
    "    \n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get) #first value is number highest scoring sentences to print\n",
    "    summary = '\\n\\n '.join(summary_sentences)\n",
    "    return summary\n",
    "\n",
    "# def run_nlp(fulltexts, searchlist):\n",
    "#     '''\n",
    "#     Input: list of key words\n",
    "#     Output: sentence with a keyword and a covid aliases summarizer thought were important\n",
    "#     '''\n",
    "#     #get summary where all you have to do is provide the words you are searching for in a list\n",
    "#     covid_alias = ['CoV', 'COVID', 'Covid', 'corona virus', 'coronavirus', 'Coronavirus', 'Corona virus'] #depending on here the results w\n",
    "#     #dictionary of sentences that contain the key terms\n",
    "#     desired_sents = {}\n",
    "#     #dictionary of sentences that contain a covid alias\n",
    "#     covid_sents = {}\n",
    "    \n",
    "#     for text in fulltexts:\n",
    "#         for sentence in text.split('. '):\n",
    "#             for i in searchlist:\n",
    "#                 if i.lower() in sentence.lower(): #using .lower changes the results dramatically\n",
    "#                     if sentence not in desired_sents.keys():\n",
    "#                         desired_sents[sentence] = sentence \n",
    "#             for j in covid_alias:\n",
    "#                 if j in sentence:\n",
    "#                     if sentence not in covid_sents.keys():\n",
    "#                         covid_sents[sentence] = sentence\n",
    "#     desired_sents = set(desired_sents.keys())\n",
    "#     covid_sents = set(covid_sents.keys())\n",
    "#     #list of the intersetion of sentences with key terms and covid aliases\n",
    "#     desired_sents = list(desired_sents.intersection(covid_sents))\n",
    "    \n",
    "#     desired_text = ''\n",
    "#     for x in desired_sents:\n",
    "#         desired_text += ' ' + x\n",
    "#     print(desired_text)\n",
    "#     result = get_summary(desired_text)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks = [('What is known about transmission, incubation, and environmental stability?', \n",
    "#         'transmission incubation environment coronavirus'),\n",
    "#         ('What do we know about COVID-19 risk factors?', 'risk factors'),\n",
    "#         ('What do we know about virus genetics, origin, and evolution?', 'genetics origin evolution'),\n",
    "#         ('What has been published about ethical and social science considerations','ethics ethical social'),\n",
    "#         ('What do we know about diagnostics and surveillance?','diagnose diagnostic surveillance'),\n",
    "#         ('What has been published about medical care?', 'medical care'),\n",
    "#         ('What do we know about vaccines and therapeutics?', 'vaccines vaccine vaccinate therapeutic therapeutics')] \n",
    "# tasks = pd.DataFrame(tasks, columns=['Task', 'Keywords'])\n",
    "\n",
    "\n",
    "# def show_task(Task):\n",
    "#     print(Task)\n",
    "#     keywords = tasks[tasks.Task == Task].Keywords.values[0]\n",
    "#     search_results = bm25_index.search(keywords, n=200)\n",
    "#     return search_results\n",
    "    \n",
    "# results = interact(show_task, Task = tasks.Task.tolist());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarizer\n",
    "Now that we have the queries working we need to summarize the text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
