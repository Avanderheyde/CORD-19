{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we know about vaccines and therapeutics?\n",
    "COVID-19 Open Research Dataset Challenge (CORD-19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we know about vaccines and therapeutics? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?**\n",
    "\n",
    "Specifically, we want to know what the literature reports about:\n",
    "\n",
    "* Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n",
    "* Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\n",
    "* Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n",
    "* Exploration of use of best animal models and their predictive value for a human vaccine.\n",
    "* Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n",
    "* Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\n",
    "* Efforts targeted at a universal coronavirus vaccine.\n",
    "* Efforts to develop animal models and standardize challenge studies\n",
    "* Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\n",
    "* Approaches to evaluate risk for enhanced disease after vaccination\n",
    "* Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 80/885 [00:00<00:01, 797.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biorxiV_medrxiv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 885/885 [00:01<00:00, 831.30it/s]\n",
      "  1%|          | 50/9118 [00:00<00:18, 490.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_use_subset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9118/9118 [00:15<00:00, 574.01it/s]\n",
      "  0%|          | 0/16959 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_license\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16959/16959 [00:29<00:00, 571.26it/s]\n",
      "  4%|▎         | 83/2353 [00:00<00:02, 821.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noncomm_use_subset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2353/2353 [00:03<00:00, 644.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get a list of stopwords from nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "dirs = ['biorxiV_medrxiv', 'comm_use_subset', 'custom_license', 'noncomm_use_subset']\n",
    "\n",
    "docs = []\n",
    "for d in dirs:\n",
    "    print(d)\n",
    "    for file in tqdm(os.listdir(f\"{d}/{d}\")):\n",
    "        filepath = f\"{d}/{d}/{file}\"\n",
    "        j = json.load(open(filepath,'rb'))\n",
    "        title = j['metadata']['title']\n",
    "        try: \n",
    "            abstract = j['abstract'][0]['text']\n",
    "        except:\n",
    "            abstract = ''\n",
    "            \n",
    "        fulltext = ''\n",
    "        for text in j['body_text']:\n",
    "            fulltext += text['text'] + \"\\n\\n\"\n",
    "        docs.append([title, abstract, fulltext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(docs, columns = ['title', 'abstract', 'fulltext'])\n",
    "#trying to keep track of titles with text so people can refer to the paper\n",
    "L1 = df['title'].values\n",
    "L2 = df['fulltext'].values\n",
    "fulltext_dict = {k:v for k,v in zip(L1,L2)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Removing Square Brackets and Extra Spaces\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    " \n",
    "    text = re.sub(r'\\{\\{[\\s\\S]*?\\}\\}', '', text)\n",
    "\n",
    "    # Remove doi links\n",
    "    #text = re.sub(r'^https://$', '',text)\n",
    "    return text\n",
    "\n",
    "def clean_spchar_digs(text):\n",
    "    # Removing special characters and digits\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text )\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def word_freq(formatted_text):\n",
    "    #creates a dictionary of words as keys and frequency as values\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    #divides the values by the maximum frequency\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
    "    \n",
    "    return word_frequencies\n",
    "\n",
    "def sent_scores(sentence_list, word_frequencies):\n",
    "    #uses the word frequencies to score the sentences by adding up the scores\n",
    "    #of the words that make up the sentence\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) <60: #limits sentence to less than 60 words\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "                    \n",
    "    return sentence_scores\n",
    "\n",
    "def get_summary(dirty_text):\n",
    "    text = clean_text(dirty_text)\n",
    "    formatted_text = clean_spchar_digs(text)\n",
    "\n",
    "    sentence_list = nltk.sent_tokenize(text)\n",
    "\n",
    "    word_frequencies = word_freq(formatted_text) \n",
    "    sentence_scores = sent_scores(sentence_list,word_frequencies)\n",
    "    \n",
    "    \n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get) #first value is number highest scoring sentences to print\n",
    "    summary = '\\n\\n '.join(summary_sentences)\n",
    "    return summary\n",
    "\n",
    "def get_improved_summary(searchlist):\n",
    "    #get summary where all you have to do is provide the words you are searching for in a list\n",
    "    covid_alias = ['CoV', 'COVID', 'Covid', 'corona virus', 'coronavirus', 'Coronavirus', 'Corona virus'] #depending on here the results w\n",
    "    desired_sents = {}\n",
    "    covid_sents = {}\n",
    "    for text in full_text.keys():\n",
    "        for sentence in text.split('. '):\n",
    "            for i in searchlist:\n",
    "                if i.lower() in sentence.lower(): #using .lower changes the results dramatically\n",
    "                    if sentence not in desired_sents.keys():\n",
    "                        desired_sents[sentence] = sentence \n",
    "            for j in covid_alias:\n",
    "                if j in sentence:\n",
    "                    if sentence not in covid_sents.keys():\n",
    "                        covid_sents[sentence] = sentence\n",
    "                        \n",
    "    desired_sents = set(desired_sents.keys())\n",
    "    covid_sents = set(covid_sents.keys())\n",
    "    desired_sents = list(desired_sents.intersection(covid_sents))\n",
    "    desired_text = ''\n",
    "    for x in desired_sents:\n",
    "        desired_text += ' ' + x\n",
    "    text = clean_text(desired_text)\n",
    "    formatted_text = clean_spchar_digs(text)\n",
    "\n",
    "    sentence_list = nltk.sent_tokenize(text)\n",
    "\n",
    "    word_frequencies = word_freq(formatted_text) \n",
    "    sentence_scores = sent_scores(sentence_list,word_frequencies)\n",
    "    \n",
    "    \n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get) #first value is number highest scoring sentences to print\n",
    "    summary = '\\n\\n '.join(summary_sentences)\n",
    "    return summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
